{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis for Indian Election 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**<br>\n",
    "The goal of this project is to do sentiment analysis for the Indian Elections. The data used is the tweets that are extracted from Twitter. The BJP and Congress are the two major political parties that will be contesting the election. The dataset will consist of tweets for both the parties. The tweets will be labeled as positive or negative based on the sentiment score obtained using Textblob library. This data will be used to build models that can classify new tweets as positive or negative. The models built are a Bidirectional RNN and GloVe word embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, SimpleRNN,Input\n",
    "from keras.models import Sequential,Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import Embedding, Flatten, Dense,Conv1D,MaxPooling1D\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools    \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, Bidirectional, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Tweepy API to access Twitter and download tweets. Tweepy supports accessing Twitter via Basic Authentication and the newer method, OAuth. Twitter has stopped accepting Basic Authentication so OAuth is now the only way to use the Twitter API.\n",
    "The below code downloads the tweets from Twitter based on the keyword that we pass. The tweets sentiment score is obtained using the textblog library. The Tweets are then preprocessed. The preprocessing involved removing emoticons, removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= '9oO3eQOBkuvCRPqMsFvnShRrq'\n",
    "consumer_secret= 'BMWGbdC05jDcsWU5oI7AouWvwWmi46b2bD8zlnWXaaRC7832ep'\n",
    "\n",
    "access_token='313324341-yQa0jL5IWmUKT15M6qM53uGeGW7FGcy1xAgx5Usy'\n",
    "access_token_secret='OyjmhcMCbxGqBQAWzq12S0zrGYUvjChsZKavMYmPCAlrE'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#file location changed to \"data/telemedicine_data_extraction/\" for clearer path\n",
    "congress_tweets = \"C:/Users/Abhishek/Election Twitter Sentiment analysis/congress_test.csv\"\n",
    "bjp_tweets = \"C:/Users/Abhishek/Election Twitter Sentiment analysis/bjp_test_new.csv\"\n",
    "\n",
    "#set two date variables for date range\n",
    "start_date = '2019-04-1'\n",
    "end_date = '2019-04-20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    " \n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "#mrhod clean_tweets()\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    " \n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    " \n",
    " \n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    " \n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    " \n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method write_tweets()\n",
    "def write_tweets(keyword, file):\n",
    "    # If the file exists, then read the existing data from the CSV file.\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    #page attribute in tweepy.cursor and iteration\n",
    "    for page in tweepy.Cursor(api.search, q=keyword,\n",
    "                              count=200, include_rts=False, since=start_date).pages(50):\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    " \n",
    "            ## check whether the tweet is in english or skip to the next tweet\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    " \n",
    "            #when run the code, below code replaces the retweet amount and\n",
    "            #no of favorires that are changed since last download.\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                continue\n",
    " \n",
    " \n",
    "           #tweepy preprocessing called for basic preprocessing\n",
    "            #clean_text = p.clean(status['text'])\n",
    " \n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(status['text'])\n",
    " \n",
    "            #pass textBlob method for sentiment calculations\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment\n",
    " \n",
    "            #seperate polarity and subjectivity in to two variables\n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    " \n",
    "            #new entry append\n",
    "            new_entry += [status['id'], status['created_at'],\n",
    "                          status['source'], status['text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    " \n",
    "            #to append original author of the tweet\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    " \n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    " \n",
    "            # hashtagas and mentiones are saved using comma separted\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    " \n",
    "            #get location of the tweet if possible\n",
    "            try:\n",
    "                location = status['user']['location']\n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    " \n",
    "            try:\n",
    "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
    "            except TypeError:\n",
    "                coordinates = None\n",
    "            new_entry.append(coordinates)\n",
    " \n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
    "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare keywords as a query for three categories\n",
    "Congress_keywords = '#IndianNationalCongress OR #RahulGandhi OR #SoniaGandhi OR #INC' \n",
    "BJP_keywords = '#BJP OR #Modi OR #AmitShah OR #BhartiyaJantaParty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates two CSV files. First saves tweets for BJP and second saves tweets for Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call main method passing keywords and file path\n",
    "write_tweets(Congress_keywords,  congress_tweets)\n",
    "write_tweets(BJP_keywords, bjp_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELING TWEETS AS POSITIVE NEGATIVE**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The tweepy libary gives out sentiment polarity in the range of -1 to +1. For our topic of election prediction the neutral tweets would be of no use as they will not provide any valuable information. Thus for simplicity purpose I have labeled tweets as only positive and negative. Tweets with polarity less than 0 will be labelled negative(0) and greater than 0 will be positive(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjp_df['polarity'] = bjp_df['polarity'].apply(lambda x: 1 if x > 0 else 0)\n",
    "congress_df['polarity'] = congress_df['polarity'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjp_df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](1.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESAMPLING THE DATA** <br>\n",
    "Since the ratio of the negative tweets to positive tweets is not proportional. Our data set is not balanced. This will create a bias while training the model. To avoid this I have resampled the data. New data was downloaded from twitter using the above procedure. For both the parties only positive tweets were sampled and appened to the main files to balance the data. After balancing the data. The count of positive and negative tweets for both the parties is as follows. The code for the resampling procedure can be found in the notebook Data_Labeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING FINAL DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [bjp, congress]\n",
    "election_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset that will be used for our analysis saved in a csv file. That file can be loaded used to run our models. The final dataset looks as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZING DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the text and keep the maximum length of the the vector 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](6.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN TEST SPLIT WITH 80:20 RATIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(.20 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING EMBEDDING MATRIX WITH HELP OF PRETRAINED MODEL: GLOVE**\n",
    "\n",
    "Word Embeddings are text converted into numbers. There are number of ways to represent the numeric forms.<br>\n",
    "\n",
    "Types of embeddings: Frequency based, Prediction based.<br>Frequency Based: Tf-idf, Co-occurrence matrix<br>\n",
    "\n",
    "Prediction-Based: BOW, Skip-gram model\n",
    "Using Pre-trained word vectors: Word2vec, Glove\n",
    "\n",
    "Word Embedding is done for the experiment with the pre trained word vector Glove.\n",
    "\n",
    "Glove version used : 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. Training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful substructures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](7.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an embedding layer using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=1000,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glove Word Embedding model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase inter-esting linear substructures of the word vector space. GloVe can be used to find relations between words like synonyms, company - product relations, zip codes, and cities etc. It is also used by the spaCy model to build semantic word em-beddings/feature vectors while computing the top list words that match with distance measures such as Cosine Similar-ity and Euclidean distance approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation():\n",
    "  input_layer = Input(shape=(1000,), dtype='int32')\n",
    "  embed_layer = embedding_layer(input_layer)\n",
    "  x = Dense(100,activation='relu')(embed_layer)\n",
    "  x = Dense(50,activation='relu', kernel_regularizer=keras.regularizers.l2(0.002))(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(50,activation='relu', kernel_regularizer=keras.regularizers.l2(0.002))(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = Dense(50, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  #x = Dense(512, activation='relu')(x)\n",
    "  #x = Dropout(0.4)(x)\n",
    "  final_layer = Dense(1, activation='sigmoid')(x)\n",
    "  opt = keras.optimizers.Adam(lr= learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  model = Model(input_layer,final_layer)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 1 Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "model_glove = model_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](9.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE BEST MODEL AND WEIGHTS for Model1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_glove.to_json()\n",
    "with open(\".\\\\SavedModels\\\\Model_glove.h5\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_glove.save_weights(\".\\\\SavedModels\\\\Weights_glove.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL1 LOSS AND ACCURAY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](10.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL1 PERFORMANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_modelacc(fit_model):\n",
    "    with plt.style.context('ggplot'):\n",
    "            plt.plot(fit_model.history['acc'])\n",
    "            plt.plot(fit_model.history['val_acc'])\n",
    "            plt.ylim(0,1)\n",
    "            plt.title(\"MODEL ACCURACY\")\n",
    "            plt.xlabel(\"# of EPOCHS\")\n",
    "            plt.ylabel(\"ACCURACY\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "    return plt.show()\n",
    "\n",
    "def plot_model_loss(fit_model):\n",
    "    with plt.style.context('ggplot'):\n",
    "            plt.plot(fit_model.history['loss'])\n",
    "            plt.plot(fit_model.history['val_loss'])\n",
    "            plt.title(\"MODEL LOSS\")\n",
    "            plt.xlabel(\"# of EPOCHS\")\n",
    "            plt.ylabel(\"LOSS\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](11.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONFUSION MATRIX**<br>\n",
    "A confusion matrix will show us the how the model predicted with respect to the acutal output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](12.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positives: 870 (Predicted True and True in reality)<br>\n",
    "True Negative: 1141(Predicted False and False in realtity)<br>\n",
    "False Positive: 33 (Predicted Positve but Negative in reality)<br>\n",
    "False Negative: 29 (Predicted Negative but Positive in reality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional RNN model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Recurrent Neural Networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously.Invented in 1997 by Schuster and Paliwal,BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state.\n",
    "\n",
    "BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 1  Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](13.png \"Title\")\n",
    "![alt text](14.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING BEST MODEL2 AND ITS WEIGHTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\".\\\\SavedModels\\\\Model_Bidir_LSTM.h5\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\".\\\\SavedModels\\\\Weights_bidir_LSTM.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 2 LOSS AND ACCURACY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](15.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](16.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 2 CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](17.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positives: 887(Predicted True and True in reality)\n",
    "True Negative: 1140(Predicted False and False in realtity)\n",
    "False Positive: 35 (Predicted Positve but Negative in reality)\n",
    "False Negative: 11 (Predicted Negative but Positive in reality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION USING THE BEST MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were compared based on the Test loss and Test Accuracy. The Bidirectional RNN performed slightly better than the GloVe model. The RNN despite its simple architec-ture performed better than the Glove model. We use the Bidirectional RNN to make the predictions for the tweets that will be used to infer election results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data on which the predictions will be made using our best model. The data for both the parties was collected using the same procedure like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_test = pd.read_csv('congress_test.csv')\n",
    "bjp_test = pd.read_csv('bjp_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took equal samples for both the files. We took 2000 tweets for Congress and 2000 for BJP. The party that gets the most number of positive votes can be infered to have the higest probablity of winning the 2019 English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_test =congress_test[:2000]\n",
    "bjp_test = bjp_test[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the tweets in the same was that were used for the Bidirectional RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_inputs = tokenze_data(congress_inputs)\n",
    "bjp_inputs = tokenze_data(bjp_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD THE BEST MODEL (BIDIRECTIONAL LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open(\".\\\\SavedModels\\\\Model_Bidir_LSTM.h5\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\".\\\\SavedModels\\\\Weights_bidir_LSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SENTIMENT PREDICTION USING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_prediction = loaded_model.predict(congress_inputs)\n",
    "bjp_prediction = loaded_model.predict(bjp_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probabilty of the outcome is greater than 0.5 for any class then the sentiment belongs to that particular class. Since we are concerned with only the count of positive sentiments. We will check the second column variables for our inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_pred = (congress_prediction>0.5)\n",
    "bjp_pred = (bjp_prediction>0.5)\n",
    "\n",
    "def get_predictions(party_pred):\n",
    "    x = 0\n",
    "    for i in party_pred:\n",
    "        if(i[1]==True):\n",
    "            x+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](18.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the training data the majority of the tweets have a negative sentiment attached to them. After feeding 2000 tweets for both the Congress and BJP. The model predicted that BJP has 660 positive tweets while Congress has 416 positive tweets.<br><br> This indicated that the contest this year would be close and the chances of BJP winning on Majority like the 2015 elections are less. This has been corraborated by the poor perfomace of the BJP in the recent state elections where the lost power in three Major Hindi speaking states Rajasthan, Madhya Pradesh and Chattishgarh. <br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUTURE SCOPE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project only, a small sample of twitter data was considered for the analysis. It is difficult to give an estimate based on the limited amount of information we had access to. For future work, we can start by increasing the size of our dataset. In addition to Twitter, data can also be obtained from websites like Facebook, News websites. Apart from these we can try different models like Bidirectional RNN with attention mechanism. We can implement BERT which is currently the state of the art for solving various Natural Language Pro-cessing problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LISCENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REFERENCES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Sepp Hochreiter and Jurgen Schmidhuber, “Long short- ¨ term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,1997.<br>\n",
    "[2] Mike Schuster and Kuldip K Paliwal, “Bidirectional recurrentneural networks,” Signal Processing, IEEE Transactions on, vol.     45, no. 11, pp. 2673–2681, 1997.<br>\n",
    "[3] Jeffrey Pennington, Richard Socher, Christopher D. Manning.GloVe: Global Vectors for Word Representation <br>\n",
    "[4] Apoorv Agarwal Boyi Xie Ilia Vovsha Owen Rambow Rebecca Passonneau Sentiment Analysis of Twitter Data <br>\n",
    "[5] Alex Graves and Jurgen Schmidhuber, “Framewise ¨ phoneme classification with bidirectional LSTM and other neural network\n",
    "    architectures,” Neural Networks, vol. 18, no. 5, pp. 602–610,2005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
